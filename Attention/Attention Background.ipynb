{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Attention vs Soft Attention\n",
    "\n",
    "**Hard Attention**: attends to a single input location.  Think of a conventional telescope only allows you to look through the peephole at a time, rather than the entire sky.\n",
    "\n",
    "- Attend to a single input location\n",
    "- Can't use gradient descent\n",
    "- Reinforcement Learning\n",
    "\n",
    "For more information, look at [Recurrent Network of Attention](../papers/Recurrent Model of Visual Attention.ipynb)\n",
    "\n",
    "**Soft Attention**: Compute a weighted combination over some inputs using an attention network.  (Think about human eye, even though it has entire view fed through, you only can focus on specific portion at a time.)\n",
    "\n",
    "- Compute a weighted combination over some inputs using an attention network\n",
    "- Can use backpropogation to train end-to-end\n",
    "\n",
    "For more information, look at [Soft Attention for Translation](../nlp/translation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to think about Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original RNN for captioning\n",
    "![](original_rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the image is treated as a state vector that is passed through the RNN.  There is no attention on each portion of the feature map (result of image through cnn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN for Captioning with Attention\n",
    "\n",
    "![](rnn_attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in this case, instead of $W \\in \\mathbb{R}^D$, we have $W \\in \\mathbb{R}^{L \\times D}$.  L refers to predefined locations on an image.\n",
    "\n",
    "At each timestep, RNN cell will output $a \\in \\mathbb{R}^L$, which will weight over the features.  The resulting vector $z \\in \\mathbb{R}^D$, will be input for next timestep.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Attention\n",
    "\n",
    "In the case of hard attention, instead of weighted sum over all $W \\in \\mathbb{R}^{L \\times D}$, the network will sample one location from $W$, $ \\in \\mathbb{R}^D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanics:  Salience\n",
    "\n",
    "### How do backpropogation work with Attention?\n",
    "\n",
    "1. What is learned in attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention and LSTMs\n",
    "\n",
    "In LSTMs, $i, f, o$ nodes are used to learn weight features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kim and Canny, “Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention,” ICCV 2017\n",
    "\n",
    "“Describing Videos by Exploiting Temporal Structure,” Li Yao et al, arXiv 2015.\n",
    "\n",
    "Attention mechanism from Show, Attend, and Tell only lets us\n",
    "softly attend to fixed grid positions ... can we do better?\n",
    "\n",
    "Gregor et al, “DRAW: A Recurrent Neural\n",
    "Network For Image Generation”, ICML 2015"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
