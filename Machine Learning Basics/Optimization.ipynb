{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "**Method 1:  Search**\n",
    "\n",
    "- Random walk in the sample space, take the one with smallest loss\n",
    "- Average random points weighted by altitude\n",
    "- More notes on [Evolution Strategies](https://blog.openai.com/evolution-strategies/), less efficient when gradients are available\n",
    "\n",
    "# Gradient Descent\n",
    "\n",
    "Suppose we write the gradient of a Loss function:\n",
    "\n",
    "$$\\nabla_{W}L(W) = [\\frac{\\partial L}{\\partial W_1}, \\frac{\\partial L}{\\partial W_2}, \\cdots \\frac{\\partial L}{\\partial W_m} ]^T$$\n",
    "\n",
    "When gradient equals to 0, it means the value L doesn't change.  This will occur when *L(W)* is at **local optimum,** or  **saddle point.**\n",
    "\n",
    "Our goal is to reach a minimum loss, finding **local minimum** is a good start.  Hence, we will update our parameters *W* to \n",
    "\n",
    "$$W^{i+1} = W^{i} - \\alpha\\nabla_W L(W)$$\n",
    "\n",
    "where *a* is the **learning rate.**\n",
    "\n",
    "## Side note:  Would gradient descent always converge?\n",
    "\n",
    "Suppose the function:\n",
    "\n",
    "$$f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$$\n",
    "\n",
    "is convex and differentiable, and the gradient is Lipschitz continuous and with constant \n",
    "\n",
    "*L > 0*.  i.e.\n",
    "\n",
    "$$\\left\\lVert\\nabla f(x) - \\nabla f(y)\\right\\rVert_2 \\leq L\\left\\lVert x - y\\right\\rVert_2$$\n",
    "\n",
    "for any *x, y*.   Then if we run gradient descent for *k* iterations with a fixed-step size *t ≤ l,* it will yield a solution *f^(k)* which satisfies\n",
    "\n",
    "$$f(x^{(k)}) - f(x^*) \\leq \\frac{\\left\\lVert x^{(0)} - x^*\\right\\rVert_2^2}{2tk}$$\n",
    "\n",
    "where *f(x*)* is the optimal value.  Gradient Descent converge with rate O(1/k)\n",
    "\n",
    "## Saddle points\n",
    "\n",
    "Saddle point is not a minimum, but gradient may stuck there because it causes gradient to be zero.\n",
    "\n",
    "There is evidence that **most zeros of the loss gradient ****are **saddles.**\n",
    "\n",
    "[Summary of Gradient Descent Variations ](./Summary-of-Gradient-Descent-Variations-96940c74-35cd-46ea-8730-d83c9d0cf3bb.csv)\n",
    "\n",
    "| Algorithm                                                         | When to use...               | Convergence Rate                               | Additional Notes                                                 | When not to use...        | \n",
    "|-------------------------------------------------------------------|------------------------------|------------------------------------------------|------------------------------------------------------------------|---------------------------| \n",
    "| Stochastic Gradient Descent                                       |                              | O(1/t)                                         | t is number of steps                                             |                           | \n",
    "| Momentum                                                          |                              |                                                |                                                                  |                           | \n",
    "| Nesterov                                                          |                              | O(1/t^2)                                       |                                                                  |                           | \n",
    "| ADAGRAD                                                           | \"Text data                   |                                                |                                                                  |                           | \n",
    "| Short task\"                                                       |                              | \"Decrease effective learning rate by 1/sqrt(t) |                                                                  |                           | \n",
    "| work well with datasets with a wide range of gradient magnitudes  |                              |                                                |                                                                  |                           | \n",
    "| has formal bound on convergence rate\"                             | Strong feature dependency    |                                                |                                                                  |                           | \n",
    "| RMSprop                                                           | Text data Long running tasks |                                                | work well with datasets with a wide range of gradient magnitudes | Strong feature dependency | \n",
    "|                                                                   |                              |                                                |                                                                  |                           | \n",
    "\n",
    "\n",
    "# Variations of Gradient Descent\n",
    "\n",
    "*How do we calculate the efficiency of Gradient Descent?*\n",
    "\n",
    "$$L = \\sum_{i=1}^N L(x_i, y_i, W)$$\n",
    "\n",
    "$$\\frac{dL}{dW} = \\sum_{i=1}^{N}\\frac{dL}{dW}(x_i, y_i, W)$$\n",
    "\n",
    "Note we are calculating the gradient through the entire dataset (N) for every update, number of calculation is proportion to *N,* which can be very expensive.\n",
    "\n",
    "In this case, there is *one* update for every **epoch**, which accounts for one forward and backward pass of entire dataset.\n",
    "\n",
    "## Mini-batch Gradient Descent\n",
    "\n",
    "Instead of computing the gradient across the entire dataset, for every update, we can only pick a fixed number of random data samples (**minibatch**)to calculate the gradient. \n",
    "\n",
    "$$g^{(t)} = \\frac{1}{m} \\sum_{j=i_1, \\cdots i_m \\in \\{1, \\cdots , N\\}} \\nabla_WL(x_j, y_j, W)$$\n",
    "\n",
    "the updates are \n",
    "\n",
    "$$W^{(t+1)} = W^{(t)} - \\alpha g^{(t)}$$\n",
    "\n",
    "    for i in range(epochs):\n",
    "        np.random.shuffle(data):\n",
    "        for batch in get_batches(data, batch_size=50):\n",
    "            params_grad = evaluate_gradient(loss_function, batch, params)\n",
    "            params = params - learning_rate * params_grad\n",
    "\n",
    "Note in this case, for every **epoch**, we will calculate *N/m* number of updates.\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "*Note:  The definition of Stochastic Gradient Descent in Lecture and Ruder Paper are different, in the lecture Stochastic Gradient Descent is Mini-batch gradient descent. Either the case, we are using a sample to estimate the average over the entire dataset.  This realization is legitimate since* \n",
    "\n",
    "$$g = \\mathbb{E}[g^{(t)}]$$\n",
    "\n",
    "Moreover, as the batch size decreases, the variance increases.\n",
    "\n",
    "Question:  Since training data is a subset of true dataset (all possible samples), isn't all gradient descent stochastic gradient descent?\n",
    "\n",
    "![](https://www.notion.so/file/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F01007c18-d1ae-4b8a-86e7-be05932c96b1%2Fgradient_descent.png)\n",
    "\n",
    "**Stochastic Gradient Descent in Tensorflow**\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(alpha) #alpha is the learning rate\n",
    "    train = optimizer.minimize(loss)\n",
    "\n",
    "# Stochastic Gradient Descent Variations\n",
    "\n",
    "## Momentum\n",
    "\n",
    "We define a momentum parameter\n",
    "\n",
    "$$p^{(t+1)} = \\mu p^{(t)} - \\alpha g^{(t)}$$\n",
    "\n",
    "$$W^{(t+1)} = W^{(t)} + p^{(t + 1)}$$\n",
    "\n",
    "Note in addition to the negative gradient , we also add a weighted momentum to the weight update.  This **momentum** is inspired by **momentum** in physics, where we are adding the direction gradient is previous moving to the new direction.\n",
    "\n",
    "**Gradient Descent with Momentum in Tensorflow**\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(alpha, momentum, use_nesterov=False)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "## Nesterov Accelerated Gradient\n",
    "\n",
    "**Nesterov** update add an additional momentum when computing the gradient, this allows the gradient to take momentum's effect into account.  \n",
    "\n",
    "$$p^{(t+1)} = \\mu p^{(t)} - \\alpha \\nabla_{W}L(W^{(t)} + \\mu p^{(t)})$$\n",
    "\n",
    "$$W^{(t + 1)} = W^{(t)} + p^{(t + 1)}$$\n",
    "\n",
    "**Nesterov Accelerated Gradient in Tensorflow**\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(alpha, momentum, use_nesterov=True)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "![](https://www.notion.so/file/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F7aae4c7c-e081-47ff-92a4-8f2d331a6760%2Fnesterov.png)\n",
    "\n",
    "**Sidenote:**  \n",
    "\n",
    "Nesterov has convergence rate of O(1/t^2) on convex optimization problems. \n",
    "\n",
    "## RMSprop\n",
    "\n",
    "RMSprop scales the gradient by inverse of a moving average.  We define RMS moving average as follows:\n",
    "\n",
    "$$s^{(t)} = \\beta s^{(t-1)} + (1 - \\beta)(g(t))^2$$\n",
    "\n",
    "The update equation is\n",
    "\n",
    "$$W^{(t+1)} = W^{(t)} - \\alpha \\frac{g^{(t)}}{\\sqrt{s^{t} + \\epsilon}}$$\n",
    "\n",
    "**RMSprop in Tensorflow**\n",
    "\n",
    "    tf.train.RMSPropOptimizer(alpha,beta=0.9,\n",
    "        momentum=0.0,\n",
    "        epsilon=1e-10,)\n",
    "\n",
    "## ADAGRAD\n",
    "\n",
    "Similar to RMSprop, ADAGRAD instead calculate the sum of squared gradients rather than moving average.\n",
    "\n",
    "$$s^{(t)} = \\sum_{j=1}^t (g^{(j)})^2$$\n",
    "\n",
    "Note: as sum of gradient grows linearly with time t, ADAGRAD decreases its effective learning rate over time.\n",
    "\n",
    "## Adam\n",
    "\n",
    "**Adam** combines RMSprop and Momentum:\n",
    "\n",
    "$$p^{(t)} = \\beta_1p^{(t-1)} + (1 - \\beta_1)g^{(t)}$$\n",
    "\n",
    "$$s^{(t)} = \\beta_2s^{(t-1)} + (1  - \\beta_2)(g^{(t)})^2$$\n",
    "\n",
    "$$W^{(t+1)} = W^{(t)} - \\alpha \\frac{p^{(t)}}{\\sqrt{s^{(t)}}}$$\n",
    "\n",
    "**ADAM** may have bias.  Suppose we initiate the moment at t = 0, the value of initial momentum is\n",
    "\n",
    "$$p^{(t)} = (1 - \\beta_1^t)g^{(t)}$$\n",
    "\n",
    "so we correct the bias with \n",
    "\n",
    "$$p_{corr}^{(t+1)} = \\frac{p^{(t)}}{1 - \\beta_1^t}$$\n",
    "\n",
    "same idea with moving average\n",
    "\n",
    "$$s_{corr}^{(t+1)} = \\frac{s^{(t)}}{1 - \\beta_2^t}$$\n",
    "\n",
    "and replace the  momentum and moving average with the corrected version.\n",
    "\n",
    "There is a better way to reduce bias (with lower variance):\n",
    "\n",
    "$$\\text{ replace } \\ \\ \\beta_2 \\ \\ \\text{ with } \\beta(t) = 1 - \\frac{1}{(t+1)^a} \\text{ where } T \\approx t^a$$\n",
    "\n",
    "**why?**\n",
    "\n",
    "# Newton's Method\n",
    "\n",
    "Newton's method employs gradient approximated by Taylor series.  **Taylor Series** is defined as \n",
    "\n",
    "$$f(x + h) = f(x) + hf'(x) + O(h^2)$$\n",
    "\n",
    "In order to find the h such that f(x + h) = 0, \n",
    "\n",
    "$$h \\approx -\\frac{f(x)}{f'(x)}$$\n",
    "\n",
    "The update equation will be\n",
    "\n",
    "$$x_{t+1} = x_t - \\frac{f'(x_t)}{f''(x_t)}$$\n",
    "\n",
    "if we want to f'(x + h) = 0.\n",
    "\n",
    "Note we only need to take one step to reach the local minimum if *f* is quadratic.\n",
    "\n",
    "For multi-variable case, the update equation will be\n",
    "\n",
    " \n",
    "\n",
    "$$x_{t+1} = x_t - H_f(x_t)^{-1}\\nabla f(x_t)$$\n",
    "\n",
    "![](https://www.notion.so/file/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fa5a98f8f-b831-406d-9431-862f56ac7014%2Fnewton.png)\n",
    "\n",
    "**Pro:**\n",
    "\n",
    "- Very fast convergence\n",
    "\n",
    "**Con:**\n",
    "\n",
    "- Too expensive (Why?)\n",
    "- Too unstable, because of big matrix inverse\n",
    "- Too clever, easy to get stuck at **saddle point.**  Perhaps randomness in SGD is a good thing? :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
