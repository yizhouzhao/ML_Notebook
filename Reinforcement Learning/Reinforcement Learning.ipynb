{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "#### What is the defining trait of Reinforment Learning?\n",
    "- There exists a loop in which the **agent** will receive feedback from the **environment**\n",
    "\n",
    "#### How do we formulate this feedback loop?\n",
    "We will define $x_t$ as the state of environment at time $t$.  (i.e. $x_t$ can be defined as a vector which each entry defines the *temperature*, *luminocity*...)\n",
    "\n",
    "However, an agent may not receive the entire $x_t$.  (A temperature gauge cannot take picture...), we will define $o_t$, which is the observation the agent makes.\n",
    "\n",
    "$u_t$ is the action agent will take at time $t$.\n",
    "\n",
    "Our goal is to learn **policy**, $\\pi_{\\theta}(u_t | o_t)$, which defines which action agent needs to take given an observati\n",
    "\n",
    "![](terms.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ｔｒａｎsition Distribution (Transition Function, Dynamics): **\n",
    "**Markov Property:**\n",
    "\n",
    "1. Observations are not conditional independent (Markov Property), states are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Functions\n",
    "\n",
    "**Rewards Function**: defines what future outcomes are desirable rather than informing what to do exactly.  So the neural network need to reason with current action to reach future reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](rl_goal1.png)\n",
    "\n",
    "We see $$p((s_{t+1}, a_{t+1}) | (s_t, a_t)) = p(s_{t+1} | s_t, a_t) \\pi_{\\theta}(a_{t+1}  s_{t+1})$$.\n",
    "\n",
    "What probabilitistic property does $\\pi_{\\theta}$ take?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta* = \\arg \\max_{\\theta} \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}[\\sum_{t}r(s_t, a_t)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infinite Horizon Case\n",
    "\n",
    "$$\\theta^* = \\arg \\max_{\\theta} \\mathbb{E}_{(s, a) \\sim p_{\\theta}(s, a)}[r(s, a)]$$\n",
    "\n",
    "### Finite Horizon Case\n",
    "\n",
    "$$\\theta^* = \\arg \\max_{\\theta} \\sum_{t=1}^T \\mathbb{E}_{(s_t, a_t) \\sim p_{\\theta}(s_t, a_t)}[r(s_t, a_t)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with Training Policy \n",
    "\n",
    "In many cases, *reward function* $r(s_i, a_i)$ is a function of $a_t$, $a_t$ may be discrete for many problems.  \n",
    "\n",
    "Or we don't know *reward function.*\n",
    "\n",
    "**Temporal Credit Assignment Problem:** how to assign appropriate weight to earlier actions to derive correct reward for current timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Gradient\n",
    "\n",
    "Instead of differentiating loss via \"rewards network\", we estimate the gradient update by enumerating trajectories, and computing the gradient along them.  \n",
    "\n",
    "Given the optimization problem:\n",
    "\n",
    "$$\\theta^* = \\arg \\max_{\\theta} \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)}\\bigg[\\sum_t r(s_t, a_t)\\bigg]$$\n",
    "\n",
    "We can evaluate the expectation as follows:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau \\sim p_{\\theta}(\\tau)} \\bigg[\\sum_t r(s_t, a_t) \\bigg] \\approx \\frac{1}{N} \\sum_{i} \\sum_{t} r（ｓ_{i,t}, a_{i,t}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct Policy Differentiation\n",
    "\n",
    "In training, $\\tau = (r_i, a_i, \\cdots r_j, a_j)$ will be sampled from the trained policy $\\pi_{\\theta}(\\tau).$  Hence, cost function will be defined as \n",
    "\n",
    "$$J(\\theta) = \\sum_{\\tau \\sim \\pi_{\\theta}(\\tau)}[r(\\tau)] = \\int \\pi_{\\theta}(\\tau) r(\\tau) d \\tau$$\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) = \\int \\nabla_{\\theta}\\pi_{\\theta}(\\tau)r(\\tau)d\\tau = \\int \\pi_{\\theta}(\\tau) \\nabla_{\\theta} \\log \\pi_{\\theta}(\\tau)r(\\tau)d\\tau = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}(\\tau)} \\bigg[\\nabla_{\\theta} \\log \\pi_{\\theta}(\\tau) r(\\tau) \\bigg]$$\n",
    "\n",
    "since\n",
    "\n",
    "$$\\pi_{\\theta}(\\tau)\\nabla_{\\theta} \\log \\pi_{\\theta}(\\tau) = \\pi_{\\theta}(\\tau) \\frac{\\nabla_{\\theta}\\pi_{\\theta}(\\tau)}{\\pi_{\\theta}(\\tau)} = \\nabla_{\\theta} \\pi_{\\theta}(\\tau)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating $\\log \\pi_{\\theta}(\\tau)$\n",
    "\n",
    "Since $$\\pi_{\\theta}(s_1, a_1, \\cdots, s_T, a_T) = p(s_1) \\prod_{t=1}^T \\pi_{\\theta}(a_t | s_t)p(s_{t+1} | s_t, a_t)$$,\n",
    "\n",
    "$$\\log \\pi_{\\theta}(\\tau) = \\log p(s_1) + \\sum_{t=1}^T \\log \\pi_{\\theta}(a_t | s_t) + \\log p(s_{t+1} | s_t, a_t)$$\n",
    "\n",
    "Since the first and last term is independent of policy, we can eliminate them.\n",
    "\n",
    "As a result,\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}(\\tau)} \\bigg[ \\bigg( \\sum_{t=1}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t)\\bigg)\\bigg(\\sum_{t=1}^T r(s_t, a_t)\\bigg)\\bigg]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Model\n",
    "\n",
    "Combining the above evaluation of policy and approximation of $J$, we define the following gradient update\n",
    "\n",
    "$$\\nabla_{\\theta}J(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^N \\bigg(\\sum_{t=1}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i,t} | s_{i, t})\\bigg)\\bigg(\\sum_{t=1}^T r(s_{i, t}, a_{i,t})\\bigg)$$\n",
    "\n",
    "This is the **policy gradient**.\n",
    "\n",
    "#### Reminder:\n",
    "\n",
    "Update equation:\n",
    "$$\\theta \\rightarrow \\theta + \\alpha \\nabla_{\\theta}J(\\theta)$$\n",
    "\n",
    "#### Algorithm:\n",
    "\n",
    "1. sample $\\{\\tau^i\\}$ from $\\pi_{\\theta}(a_t | s_t)$ from the policy\n",
    "2. calculate $\\nabla_{\\theta}J(\\theta)$\n",
    "3. update via update equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Action Spaces\n",
    "\n",
    "If the action $a_t$ is discrete,\n",
    "\n",
    "$$\\nabla_{\\theta}J(\\theta) \\approx \\frac{1}{N}\\sum_{i=1}^N \\bigg(\\sum_{t=1}^T \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{i,t} | s_{i, t})\\bigg)$$ is analogous to \n",
    "gradient of cross-entropy action prediction loss.  (How different?)\n",
    "\n",
    "So,\n",
    "1. the loss for each trajectory is weighted by trajectory's reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Actions with Gaussian Policies\n",
    "\n",
    "In this case, policy is sampled from gaussian distribution:\n",
    "\n",
    "$$\\pi_{\\theta}(a_t | s_t) = \\mathcal{N}\\big(f_{\\text{neural network}(s_t);} \\Sigma\\big)$$\n",
    "\n",
    "$$\\log \\pi_{\\theta}(a_t | s_t) = - \\frac{1}{2}\\left\\lVert f(s_t) - a_t \\right\\rVert_{\\Sigma}^2 + \\text{const}$$\n",
    "\n",
    "$$\\nabla_{\\theta}\\log \\pi_{\\theta}(a_t | s_t) = - \\frac{1}{2}\\Sigma^{-1}(f(s_t) - a_t)\\frac{df}{d\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Variance\n",
    "\n",
    "Note, policy at time $t'$ cannot affect reward at time $t$ when $t < t'$.  So rewards part of policy gradient can be defined as \n",
    "\n",
    "$$\\sum_{t'=t}^T r(s_{i,t'}, a_{i,t'})$$\n",
    "\n",
    "Note we are only summing over the future rewards.  This is known as **rewards to go**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines\n",
    "\n",
    "We shall subtract rewards $r(\\tau)$ by $b$. (Why do we do that?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy policy gradient with Importance Sampling\n",
    "\n",
    "## Trust Region Policy Optimization\n",
    "\n",
    "## Proximal Policy Optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
