{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RCNN\n",
    "\n",
    "Progress:  Need to finish training pipeline for FasterRCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input for Fast RCNN is an image and a list of regional proposals.  \n",
    "\n",
    "We will treat each image as $x_i \\in \\mathbb{R}^{H,W,C}$ and regional proposal as a list of $[\\cdots (r,c,h,w)_i \\cdots ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "![fastrcnn](fastrcnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Additions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoI Pooling Layer\n",
    "\n",
    "**RoI pooling**, given non-uniform sized windows, output a fixed sized feature map.  \n",
    "\n",
    "- Inputs:\n",
    "    - Feature Maps (result of convolution/max pooling layers)\n",
    "    - Region Proposals (N, 5), where N is number of proposals and each column represent: \n",
    "        - image id\n",
    "        - top left corner coordninates (r, c)\n",
    "        - height and width of regional proposal (h, w)\n",
    "        \n",
    "- Hyperparameter: \n",
    "    - (H, W): fixed sized output feature map (i.e. output dimension will be (H, W, F))\n",
    "- Process\n",
    "    - Pull out regional proposal:  ((r, c), (r + h, c), (r, w+c), (r+h, w+c))\n",
    "    - Divide the grid into (H, W) sub-grids, each grid is (h/H, w/W) size\n",
    "    - perform max pooling at each subgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement with Respect to RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Incorporated Training in one algorithm:  (Is this good or bad?)\n",
    "2. RoI (Region of Interest Pooling)\n",
    "3. Same Feature Map for all proposed regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "Input for Faster RCNN is an image.  No predefined regional proposal is needed as input.  \n",
    "\n",
    "We will treat each image as $x_i \\in \\mathbb{R}^{H,W,C}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "![fasterrcnn](fasterrcnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Additions:\n",
    "\n",
    "### Region Proposal Network\n",
    "\n",
    "Input is an image and outputs a set of rectangular object proposals.\n",
    "\n",
    "#### General Idea:\n",
    "\n",
    "Slide a small network over the convolutional feature map output (from the last shared layer).  For each n x n spatial window, we will generate\n",
    "1. k anchor boxes of predefined size\n",
    "2. Cast n x n into a lower dimensional feature vector.  \n",
    "3. The lower dimensional feature vector will be fed into two 1 x 1 (or fully connected) network, results in two output vectors\n",
    "    - $\\mathbb{R}^{2k}$ - which is the score on how likely is the item an object\n",
    "    - $\\mathbb{R}^{4k}$ - for each anchor (out of k), will output (r, c, h, w) for input into RoI network\n",
    "    \n",
    "*Note*: Each anchor is fixed size, every anchor will produce various (r, c, h, w)* outputs that may deviated from anchor.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Faster RCNN with Provided Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FasterRCNN Maps\n",
    "\n",
    "1. Preprocess\n",
    "2. Convolutional Network that produces feature maps\n",
    "    - can use a conventional one (ImageNet, Inception, ResNet)\n",
    "    - Input: Image $\\in \\mathbb{R}^{N, H, W, C}$\n",
    "    - Output: Feature Map $\\in \\mathbb{R}^{}$\n",
    "3. RPN Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import object_detection\n",
    "from object_detection.core import model, box_list\n",
    "import slim\n",
    "from nets import resnet_utils, resnet_v1\n",
    "from object_detection.utils import ops\n",
    "\n",
    "# from object_detection.model_lib import create_estimator_and_inputs, create_train_and_eval_specs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.predictors.heads import box_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "This library is from google's tensorflow research model library.  Each architecture is defined by a MetaArch class.  For Detection Models, the MetaArch contains the following methods:\n",
    "\n",
    "1. Preprocess\n",
    "2. predict\n",
    "3. loss\n",
    "\n",
    "such that the training pipeline will move from 1 -> 2 -> 3.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters, Hyperparameters, and Inputs\n",
    "\n",
    "This note focus on model building rather than input building.  For more information, look at this [tutorial for datainput]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will define a .config file in the project folder\n",
    "pipeline_config_path = \"faster_rcnn.config\"\n",
    "\n",
    "# The following library is from object detection of tensorflow research\n",
    "\n",
    "from object_detection.utils import config_util\n",
    "from object_detection import inputs\n",
    "\n",
    "get_configs_from_pipeline = config_util.get_configs_from_pipeline_file\n",
    "\n",
    "merge_external_params_with_configs = config_util.merge_external_params_with_configs\n",
    "create_train_input_fn = inputs.create_train_input_fn\n",
    "create_eval_input_fn = inputs.create_eval_input_fn\n",
    "create_predict_input_fn = inputs.create_predict_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = get_configs_from_pipeline(pipeline_config_path)\n",
    "\n",
    "model_config = configs['model']\n",
    "train_config = configs['train_config']\n",
    "train_input_config = configs['train_input_config']\n",
    "eval_config = configs['eval_config']\n",
    "eval_input_config = configs['eval_input_config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = create_train_input_fn(\n",
    "    train_config=train_config,\n",
    "    train_input_config=train_input_config,\n",
    "    model_config=model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build image_resizer function\n",
    "from object_detection.builders import image_resizer_builder\n",
    "\n",
    "image_resizer = image_resizer_builder.build(model_config.faster_rcnn.image_resizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run_config \n",
    "# config = tf.estimator.RunConfig(model_dir=\"model/\")\n",
    "\n",
    "# # Hparams\n",
    "# hparams = model_hparams.create_hparams()\n",
    "# # pipeline_config_path\n",
    "# pipeline_config = \"faster_rcnn.config\"\n",
    "# # train_steps\n",
    "# train = 100\n",
    "# # eval_steps\n",
    "# eval_steps = 100\n",
    "\n",
    "# train_and_eval_dict = create_estimator_and_inputs(\n",
    "#     run_config =config,\n",
    "#     hparams=hparams,\n",
    "#     pipeline_config_path = pipeline_config,\n",
    "#     train_steps = train,\n",
    "#     eval_steps = eval_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Map Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_feature_maps(inputs, scope, args):\n",
    "    \"\"\"\n",
    "    Given a input (image) of dimension:  [Batch, Height, Width, Channels], output \n",
    "    a feature map of dimension: [Batch, Height, Width, Depth]\n",
    "    \"\"\"\n",
    "    IMAGE_MIN_HEIGHT = 33\n",
    "    IMAGE_MIN_WIDTH = 33\n",
    "    \n",
    "    shape = inputs.get_shape()\n",
    "    if len(shape.as_list()) != 4:\n",
    "        raise ValueError('`preprocessed_inputs` must be 4 dimensional, got a '\n",
    "                               'tensor of shape %s' % shape)\n",
    "        \n",
    "    height, width = tf.shape(inputs)[1], tf.shape(inputs)[2] \n",
    "    shape_assert = tf.Assert(tf.logical_and(tf.greater_equal(height, IMAGE_MIN_HEIGHT),\n",
    "            tf.greater_equal(width, IMAGE_MIN_WIDTH)),\n",
    "                             \n",
    "        ['image size must at least be %s in both height and width.' % IMAGE_MIN_HEIGHT])\n",
    "\n",
    "    resnet_scope = resnet_utils.resnet_arg_scope(\n",
    "                batch_norm_epsilon=1e-5,\n",
    "                batch_norm_scale=True,\n",
    "                weight_decay=args._weight_decay)\n",
    "    \n",
    "    with tf.control_dependencies([shape_assert]):\n",
    "        with slim.arg_scope(resnet_scope):\n",
    "            with tf.variable_scope(args.architecture, reuse=args.reuse_weights) as var_scope:\n",
    "            \n",
    "                _, activations = resnet_v1(\n",
    "                  inputs,\n",
    "                  num_classes=None,\n",
    "                  is_training=args.train_batch_norm,\n",
    "                  global_pool=False,\n",
    "                  output_stride=args.first_stage_features_stride,\n",
    "                  spatial_squeeze=False,\n",
    "                  scope=var_scope)\n",
    "\n",
    "    handle = scope + '/%s/block3' % args.architecture\n",
    "    return activations[handle]\n",
    "\n",
    "\n",
    "def anchor_generator(map_height, map_width):\n",
    "    scales = (0.5, 1.0, 2.0)\n",
    "    aspect_ratios = (0.5, 1.0, 2.0)\n",
    "    base_anchor_size = (256, 256)\n",
    "    anchor_stride = (16, 16)\n",
    "    anchor_offset = (0, 0)\n",
    "    \n",
    "    scales_grid, aspect_ratios_grid = ops.meshgrid(scales, aspect_ratios)\n",
    "    # scales_grid:  [...[0.5, 1.0, 2.0]...] (think for each aspect_ratio,\n",
    "    # we have a scale list)\n",
    "    scales = tf.reshape(scales_grid, [-1])\n",
    "    aspect_ratios = tf.reshape(aspect_ratios_grid, [-1])\n",
    "    \n",
    "    sqrt_ratio = tf.sqrt(aspect_ratios)\n",
    "    heights = scales / ratio_sqrts * base_anchor_size[0]\n",
    "    widths = scales * ratio_sqrts * base_anchor_size[1]\n",
    "\n",
    "    # Get a grid of box centers\n",
    "    y_centers = tf.to_float(tf.range(map_height))\n",
    "    y_centers = y_centers * anchor_stride[0] + anchor_offset[0]\n",
    "    x_centers = tf.to_float(tf.range(grid_width))\n",
    "    x_centers = x_centers * anchor_stride[1] + anchor_offset[1]\n",
    "    x_centers, y_centers = ops.meshgrid(x_centers, y_centers)\n",
    "    \n",
    "    widths_grid, x_centers_grid = ops.meshgrid(widths, x_centers)\n",
    "    heights_grid, y_centers_grid = ops.meshgrid(heights, y_centers)\n",
    "    \n",
    "    bbox_centers = tf.stack([y_centers_grid, x_centers_grid], axis=3)\n",
    "    bbox_sizes = tf.stack([heights_grid, widths_grid], axis=3)\n",
    "    centers = tf.reshape(bbox_centers, [-1, 2])\n",
    "    sizes = tf.reshape(bbox_sizes, [-1, 2])\n",
    "    \n",
    "    \n",
    "    bbox_corners = tf.concat([centers - .5 * sizes, centers + .5 * sizes], 1)\n",
    "    anchors = box_list.BoxList(bbox_corners)\n",
    "    \n",
    "    anchor_indices = tf.zeros([anchors.num_boxes()])\n",
    "    anchors.add_fields(\"feature_map_index\", anchor_indices)\n",
    "    return anchors\n",
    "    \n",
    "def extract_rpn_features(feature_map_input, feature_map_shape):\n",
    "    # we crop feature_map_input\n",
    "    with slim.arg_scope():\n",
    "        predict_feature = slim.conv2d(feature_map_input,\n",
    "                   first_stage_box_depth,\n",
    "                   kernel_size=[kernel_size, kernel_size]\n",
    "                   rate=atrous_rate,\n",
    "                   activation_fn = tf.nn.relu6)\n",
    "    return predict_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_box_predictor(is_training,\n",
    "                        num_classes,\n",
    "                        predictor_config,\n",
    "                        conv_hyperparams_fn\n",
    "                       ):\n",
    "    \n",
    "    box_prediction_head = box_head.ConvolutionalBoxHead(\n",
    "        is_training=is_training,\n",
    "        box_code_size=predictor_config.box_code_size,\n",
    "        kernel_size=predictor_config.kernel_size,\n",
    "        use_depthwise=True)\n",
    "    class_prediction_head = class_head.ConvolutionalClassHead(\n",
    "        is_training=is_training,\n",
    "        num_classes=num_classes,\n",
    "        use_dropout=False,\n",
    "        dropout_keep_prob=predictor_config.dropout_keep_prob,\n",
    "        kernel_size=predictor_config.kernel_size,\n",
    "        apply_sigmoid_to_scores=True,\n",
    "        class_prediction_bias_init=predictor_config.class_prediction_bias_init,\n",
    "        use_depthwise=True)\n",
    "    return convolutional_box_predictor.ConvolutionalBoxPredictor(\n",
    "      is_training=is_training,\n",
    "      num_classes=num_classes,\n",
    "      box_prediction_head=box_prediction_head,\n",
    "      class_prediction_head=class_prediction_head,\n",
    "      other_heads={},\n",
    "      conv_hyperparams_fn=conv_hyperparams_fn,\n",
    "      num_layers_before_predictor=predictor_config.num_layers_before_predictor,\n",
    "      min_depth=predictor_config.min_depth,\n",
    "      max_depth=predictor_config.max_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_detection.utils import shape_utils\n",
    "\n",
    "class FasterRCNNMetaArch(model.DetectionModel):\n",
    "    \"\"\"\n",
    "    This is a simpler implementation of Faster RCNN using DetectionModel\n",
    "    Architecture \n",
    "       \n",
    "    (From DetectionModel documentation):\n",
    "    \n",
    "    Training process - \n",
    "        input -> preprocess (need to implement) \n",
    "        -> predict (need to implement) \n",
    "        -> loss (need to implement)\n",
    "        -> output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, resizer, model_config):\n",
    "        super(FasterRCNNMetaArch, self).__init__(num_classes=num_classes)\n",
    "        self.image_resizer = resizer\n",
    "        self.model_config = model_config\n",
    "        \n",
    "        \n",
    "    def preprocess(self, inputs):\n",
    "        if inputs.dtype is not tf.float32:\n",
    "            raise ValueError('`preprocess` expects a tf.float32 tensor')\n",
    "        \n",
    "        with tf.name_scope('Preprocessor'):\n",
    "            outputs = shape_utils.static_or_dynamic_map_fn(\n",
    "            self._image_resizer_fn, elems=inputs, dtype=[tf.float32, tf.int32],\n",
    "            parallel_iterations=self._parallel_iterations)\n",
    "            \n",
    "        resized_inputs = outputs[0]\n",
    "        true_image_shapes = outputs[1]\n",
    "        return (self._feature_extractor.preprocess(resized_inputs),\n",
    "              true_image_shapes)\n",
    "    \n",
    "    def predict(self, preprocessed_inputs):\n",
    "        \"\"\"    \n",
    "        Args:\n",
    "          preprocessed_inputs: a [batch, height, width, channels] float tensor\n",
    "            representing a batch of images.\n",
    "          true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
    "            of the form [height, width, channels] indicating the shapes\n",
    "            of true images in the resized images, as resized images can be padded\n",
    "            with zeros.\n",
    "\n",
    "        Returns:\n",
    "          prediction_dict: a dictionary holding \"raw\" prediction tensors:\n",
    "            1) rpn_box_predictor_features: A 4-D float32 tensor with shape\n",
    "              [batch_size, height, width, depth] to be used for predicting proposal\n",
    "              boxes and corresponding objectness scores.\n",
    "            2) rpn_features_to_crop: A 4-D float32 tensor with shape\n",
    "              [batch_size, height, width, depth] representing image features to crop\n",
    "              using the proposal boxes predicted by the RPN.\n",
    "            3) image_shape: a 1-D tensor of shape [4] representing the input\n",
    "              image shape.\n",
    "            4) rpn_box_encodings:  3-D float tensor of shape\n",
    "              [batch_size, num_anchors, self._box_coder.code_size] containing\n",
    "              predicted boxes.\n",
    "            5) rpn_objectness_predictions_with_background: 3-D float tensor of shape\n",
    "              [batch_size, num_anchors, 2] containing class\n",
    "              predictions (logits) for each of the anchors.  Note that this\n",
    "              tensor *includes* background class predictions (at class index 0).\n",
    "            6) anchors: A 2-D tensor of shape [num_anchors, 4] representing anchors\n",
    "              for the first stage RPN (in absolute coordinates).  Note that\n",
    "              `num_anchors` can differ depending on whether the model is created in\n",
    "              training or inference mode.\n",
    "\n",
    "        Raises:\n",
    "          ValueError: If `predict` is called before `preprocess`.\n",
    "        \"\"\" \n",
    "    \n",
    "        # ResNet extract block3 feature\n",
    "        feature_map = conv_feature_maps(inputs, scope, self.model_config)\n",
    "\n",
    "        feature_map_shape = tf.shape(feature_map)\n",
    "\n",
    "        # Generate anchors\n",
    "        anchors = anchor_generator(feature_map_shape[0], feature_map_shape[1])\n",
    "\n",
    "        # At this point, we have two feature maps to look at.  One is\n",
    "        # prediction network, where for each location, we will predict anchor number\n",
    "        # of objectiveness score, this is achieved using box predictor\n",
    "        box_predictor = build_box_predictor(is_training,\n",
    "                        num_classes,\n",
    "                        predictor_config,\n",
    "                        conv_hyperparams_fn)\n",
    "        \n",
    "        box_predict = box_predictor.predict(feature_map)\n",
    "        box_objectiveness = box_predict[box_predict.CLASS_PREDICTIONS_WITH_BACKGROUND]\n",
    "        box_encoding = box_predict[box_predict.BOX_ENCODINGS]\n",
    "        clip_window = tf.to_float(tf.stack([0, 0, image_shape[1], image_shape[2]]))\n",
    "        anchors_boxlist = box_list_ops.clip_to_window(anchors_boxlist, clip_window, filter_nonoverlapping=False)\n",
    "\n",
    "        prediction_dict = {\n",
    "            'rpn_box_predictor_features': rpn_box_predictor_features,\n",
    "            'rpn_features_to_crop': rpn_features_to_crop,\n",
    "            'image_shape': image_shape,\n",
    "            'rpn_box_encodings': rpn_box_encodings,\n",
    "            'rpn_objectness_predictions_with_background':\n",
    "            rpn_objectness_predictions_with_background,\n",
    "            'anchors': self._anchors.get()\n",
    "        }\n",
    "\n",
    "    \n",
    "        return prediction_dict\n",
    "    \n",
    "    def loss(self, prediction_dict, true_image_shapes, scope=None):\n",
    "        \"\"\"Compute scalar loss tensors given prediction tensors.\n",
    "\n",
    "        If number_of_stages=1, only RPN related losses are computed (i.e.,\n",
    "        `rpn_localization_loss` and `rpn_objectness_loss`).  Otherwise all\n",
    "        losses are computed.\n",
    "\n",
    "        Args:\n",
    "          prediction_dict: a dictionary holding prediction tensors (see the\n",
    "            documentation for the predict method.  If number_of_stages=1, we\n",
    "            expect prediction_dict to contain `rpn_box_encodings`,\n",
    "            `rpn_objectness_predictions_with_background`, `rpn_features_to_crop`,\n",
    "            `image_shape`, and `anchors` fields.  Otherwise we expect\n",
    "            prediction_dict to additionally contain `refined_box_encodings`,\n",
    "            `class_predictions_with_background`, `num_proposals`, and\n",
    "            `proposal_boxes` fields.\n",
    "          true_image_shapes: int32 tensor of shape [batch, 3] where each row is\n",
    "            of the form [height, width, channels] indicating the shapes\n",
    "            of true images in the resized images, as resized images can be padded\n",
    "            with zeros.\n",
    "          scope: Optional scope name.\n",
    "\n",
    "        Returns:\n",
    "          a dictionary mapping loss keys (`first_stage_localization_loss`,\n",
    "            `first_stage_objectness_loss`, 'second_stage_localization_loss',\n",
    "            'second_stage_classification_loss') to scalar tensors representing\n",
    "            corresponding loss values.\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.name_scope(scope, \"Loss\", prediction_dict.values()):\n",
    "\n",
    "          (groundtruth_boxlists, groundtruth_classes_with_background_list,\n",
    "           groundtruth_masks_list, groundtruth_weights_list\n",
    "          ) = self._format_groundtruth_data(true_image_shapes)\n",
    "          loss_dict = self._loss_rpn(\n",
    "              prediction_dict['rpn_box_encodings'],\n",
    "              prediction_dict['rpn_objectness_predictions_with_background'],\n",
    "              prediction_dict['anchors'], groundtruth_boxlists,\n",
    "              groundtruth_classes_with_background_list, groundtruth_weights_list)\n",
    "\n",
    "        return loss_dict        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_rcnn_model = FasterRCNNMetaArch(model.DetectionModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "Input for Fast RCNN is an image and a list of regional proposals.  \n",
    "\n",
    "We will treat each image as $x_i \\in \\mathbb{R}^{H,W,C}$ and regional proposal as a list of $[\\cdots (r,c,h,w)_i \\cdots ]$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
