{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology\n",
    "\n",
    "**Semantics**: concerned with meaning of texts\n",
    "1. **Lexical Semantics**:  focuses on the meaning of individual words\n",
    "2. **Compositional Semantics**: meaning depends on the words, and on how they are combined.\n",
    "\n",
    "**Syntax**: \n",
    "\n",
    "**Paradigmatic Similarity**:\n",
    "\n",
    "### Approaches\n",
    "**Propositional or formal semantics**: Block of text is converted into a formula in logical language\n",
    "- \"dog bites man\" $\\rightarrow$ bites(dog, man)\n",
    "- bites(\",\") is a binary relation\n",
    "- Probaility can be attached to a relation\n",
    "\n",
    "#### Pros:\n",
    "- Allows for logical inferences\n",
    "- Good for structured language domains (where causal structure is prevalent)\n",
    "\n",
    "**Vector Representation**: Texts are embedded into a high-dimensional space\n",
    "- vec(\"dog bites man\") = $(0.2, -0.3, 1.5, \\cdots ) \\in \\mathbb{R}^n$\n",
    "\n",
    "#### Pros:\n",
    "\n",
    "1. Allows to measure word similarity\n",
    "\n",
    "#### Cons\n",
    "1. Hard to compose semantic logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Embedding of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis\n",
    "\n",
    "Given a set of documents, referred to as **corpus**, we shall construct a matrix $T \\in \\mathbb{Z}^{N \\times M}$ where\n",
    "\n",
    "$T_{ij}$ is the count of word $j$ in document $i.$\n",
    "\n",
    "1. We shall first compute L2 Approximation of $T$.\n",
    "- $T \\approx U^T V$ \n",
    "\n",
    "![](lsa1.png)\n",
    "or\n",
    "- $T \\approx U^T S ^V$\n",
    "![](lsa2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "1. How do one interpret $V, S, U$?\n",
    "2. What is the difference between $U^T V$ and $U^TSV$\n",
    "\n",
    "$v = Vt$ is an embedding of the document in the latent space.\n",
    "\n",
    "$t' = V^Tv = V^TVt$ is the decoding of the document from its embedding.\n",
    "\n",
    "3. Why is V^T used to reconstruct from the embedded space?\n",
    "\n",
    "An SVD (Singular Value Decomposition) factorization gives the best possible reconstructionsof the documents 푡′from their embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder in LSA Perspective\n",
    "\n",
    "![autoencode](autoencode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](lsadeepnetwork.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgrams\n",
    "![](skip-gram.png)\n",
    "\n",
    "### Continuous Bag of Words\n",
    "![](cbow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "![](w2v-ex.png)\n",
    "\n",
    "**In this case**: context window $= 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW Input - $(\\cdots \\text{(\"the\", \"quick\"), (\"brown\", \"quick\")} \\cdots)$\n",
    "### Skipgram Input - $(\\cdots \\text{(\"quick\", \"the\"), (\"quick\", \"brown\")} \\cdots)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Word2Vec in a Network\n",
    "\n",
    "![](w2v-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply **CBOW** and **Skipgram** to the neural network, we can represent $x$ and $y$ as one hot vector.  If $x$ and $y$ are one hot vector, then using softmax makes sense because it will output the probability predicted value will be $y$.  \n",
    "\n",
    "Other times, we may need to create an embedding for $x$ and $y$.  Such the case, there are two embedding matrices:  $V$ and $U$.  \n",
    "\n",
    "$u$ will be output embedding vector\n",
    "$v$ will be input embedding vector.\n",
    "\n",
    "Softmax function for this network (and skipgram) will be defined as:\n",
    "\n",
    "$$p(j \\ | \\ i) = \\frac{\\exp(u_j^Tv_i)}{\\sum_{k=1}^V \\exp(u_k^Tv_i)}$$\n",
    "\n",
    "$j$ is the output word\n",
    "$i$ is the input word\n",
    "$k$ allows us to range through the word context.\n",
    "\n",
    "What would the softmax function for (cbow) be?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition\n",
    "\n",
    "“Linguistic Regularities in Continuous Space Word Representations\"\n",
    "\n",
    "![](composition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros of Word2Vec\n",
    "\n",
    "Local context allows us to capture more information about relations.\n",
    "\n",
    "![](relation.png)\n",
    "\n",
    "### Cons of Word2Vec\n",
    "- Cross entropy loss may put emphasis on small combination of word/contexts (Why?)\n",
    "- Very expensive to normalize the softmax over all words\n",
    "- Uses heuristic down-weighting of frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove\n",
    "\n",
    "### Co-occurrence Matrices\n",
    "\n",
    "To combine global context and local context information, **Glove** uses **Co-occurrence Matrices**. \n",
    "\n",
    "$C_{ij}$ counts the number of documents containing both $i$ and $j$ given a **window size**.\n",
    "\n",
    "Glove minimizes:\n",
    "\n",
    "$$J(\\theta) = \\sum_{i,j=1}^V f(C_{ij})(u_i^Tv_j + b_i + \\tilde{b_j} - \\log C_{ij})^2$$\n",
    "\n",
    "$f$ is a function that satisfies:\n",
    "1. $f(0) = 0$\n",
    "2. $f(x)$ is non-decreasing\n",
    "3. $f(x)$ \"saturates\" -- not too large for large $x$\n",
    "\n",
    "**Example** of $f$ is:\n",
    "\n",
    "$$f(x) = \\begin{cases}\n",
    "          (x/x_{\\max})^{\\alpha} & \\text{if} x < x_{\\max} \\\\\n",
    "          1                     & \\text{otherwise}\n",
    "         \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Thought Vector\n",
    "\n",
    "**Skip-thought** vectors utilizes RNNs to predict the next and previous sentences.\n",
    "\n",
    "So, **RNN** is applied to each word, and the output state vector of the boundary layer (usually end of a phrase that is considered as a unit) will be embedding.\n",
    "\n",
    "After we train to create the embedding, then the recurrent network can be used as a encoder.\n",
    "\n",
    "**How do we encode larger unit of texts?**\n",
    "\n",
    "**Why don't we need backpropogation to train RNN?**\n",
    "\n",
    "**Why can we represent hte encoder as a (truly) recurrent network?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: READ Skip Thought paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "1. SICK semantic relatedness scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Siamese Network for Semantic Relatedness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](siamese.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a pair of sentences, $a$ and $b$ with similarity label $y$ and parameter shared between two networks, we shall train the network, while expanding the data by substituting random words with wordnet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
