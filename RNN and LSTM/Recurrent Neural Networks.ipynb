{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurrent Neural Network** introduced cycles and notion of time.  \n",
    "![rnn](rnn.png)\n",
    "\n",
    "We can unroll the RNN as follows:\n",
    "\n",
    "![unrolled_rnn](unrolled_rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to compute the forward and backward propogation for recurrent neural network?\n",
    "\n",
    "For every timestep $t$, $h_t$ and $y_t$ is computed with the following formulas:\n",
    "\n",
    "$$h_t = \\text{tanh}(W_{hh}h_{t-1} + W_{xh}x_t)$$\n",
    "\n",
    "$$y_t = W_{hy}h_t$$\n",
    "\n",
    "Note same function and same parameter is used at every timestep.  Moreoever, we need to across time before calculating higher level parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Recall for non-recurrent neural network, the dimension of data input is $x \\in \\mathbb{R}^{N, M}$ where $N$ is number of data points and $M$ is number of features.  \n",
    "\n",
    "For recurrent neural network, we will add an additional dimension $T$, which represents the *timestamp*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Example network for image captioning in tensorboard)\n",
    "from coco_input import *\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import pandas as pd\n",
    "data_dir = '/home/karen/workspace/data/cocodataset/'\n",
    "coco_data = CocoCaptionData(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_data, features = coco_data.sample_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = os.path.join(data_dir, \"word_to_id.csv\")\n",
    "word_map = pd.read_csv(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_map = list([v for i, v in word_map.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35848\n"
     ]
    }
   ],
   "source": [
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption_labels = pd.read_csv(coco_data.labels_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(caption_labels.values[0])\n",
    "# image_id = [i[-1] for i in caption_labels.values]\n",
    "# caption_ids = [np.trim_zeros(i[1:-2]) for i in caption_labels.values]\n",
    " \n",
    "# print(image_id[0])\n",
    "# print(caption_ids[0])\n",
    "# file_name = np.array([np.array(features[i]) for i in image_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = tf.data.Dataset.from_tensor_slices((dict(filename=file_name), caption_ids))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_id = level[-1]\n",
    "# s = np.trim_zeros(level[1:-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_vocab(caption_data):\n",
    "#     vocab_data = set()\n",
    "#     for c in caption_data:\n",
    "#         for v in c.split(\" \"):\n",
    "#             v = v.lower()\n",
    "#             v = v.split(\".\")[0]\n",
    "#             vocab_data.add(v)\n",
    "#     return list(vocab_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = get_vocab(caption_data[\"caption\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(np.array(l))\n",
    "# df.to_csv(os.path.join(data_dir, \"word_to_id.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_map = {w:i for i, w in enumerate(l)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caption_array = []\n",
    "# for c in caption_data[\"caption\"]:\n",
    "#     s = []\n",
    "#     for v in c.split(\" \"):\n",
    "#         v = v.lower()\n",
    "#         v = v.split(\".\")[0]\n",
    "        \n",
    "#         s.append(int(word_map[v]))\n",
    "#     length = len(s)\n",
    "#     s = np.pad(np.array(s), (0, 180-length), \"constant\")\n",
    "#     caption_array.append(np.array(s))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, t in enumerate(caption_data[\"image_id\"]):\n",
    "#     caption_array[i] = np.append(caption_array[i], [t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_caption = pd.DataFrame.from_dict(caption_array)\n",
    "# img_caption.to_csv(os.path.join(data_dir, \"caption_int.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randint = np.random.randint(len(features))\n",
    "# image_id = caption_data[\"image_id\"][randint]\n",
    "# caption = caption_data[\"caption\"][randint]\n",
    "# sample_image_id = features\n",
    "# # img_url = features[sample_image_id]\n",
    "# filename = sample_image_id[image_id]\n",
    "# # d = coco_data.dict_data\n",
    "# caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = PIL.Image.open(img)\n",
    "plt.imshow(im)\n",
    "print(im.size)\n",
    "plt.show()\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn = coco_data.coco_input(data_dir, True, False, False, 1, 500, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'filename': <tf.Tensor 'IteratorGetNext:0' shape=(128,) dtype=string>,\n",
       "  'img': <tf.Tensor 'IteratorGetNext:1' shape=(128, 224, 224, 3) dtype=float32>},\n",
       " <tf.Tensor 'IteratorGetNext:2' shape=(128, 180) dtype=int64>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_fn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Models with RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![onetomany](12many.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image captioning**: image captioning would require one input and output frome many recurrent cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet for Image Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resnet50 import resnet_model_no_last_layer\n",
    "from run_model import get_available_gpus,run_model_fn,per_device_batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_rnn(inputs, labels, is_training, use_batchnorm, data_format, name=\"resnet_rnn\"):\n",
    "    feature_map = resnet_model_no_last_layer(inputs, is_training, use_batchnorm, data_format, name)\n",
    "    # Output a N X 2048 Tensor\n",
    "    initial_state = feature_map\n",
    "    _, state_size = initial_state.shape\n",
    "    print(state_size)\n",
    "    print(VOCAB_SIZE)\n",
    "    rand = tf.random_uniform([VOCAB_SIZE, 2048], -1.0, 1.0)\n",
    "    embeddings = tf.Variable(rand)\n",
    "\n",
    "    \n",
    "    embed = tf.nn.embedding_lookup(embeddings, labels)\n",
    "\n",
    "    print(embed.shape)\n",
    "    return rnn_zero_state(state_size, embed)\n",
    "    \n",
    "def rnn_zero_state(state_size, data):\n",
    "    cell1 = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "    cell2 = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "    multi_cell = tf.nn.rnn_cell.MultiRNNCell([cell1, cell2])\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell=multi_cell, inputs=data, dtype=tf.float32)\n",
    "    return outputs, state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model_fn(features, labels, mode, model, name, use_batchnorm, is_hydra, data_format):\n",
    "    features = features[\"img\"]\n",
    "    # Generate a summary node for the images\n",
    "    tf.summary.image('images', features, max_outputs=6)\n",
    "    \n",
    "    outputs, state = model(features, labels, mode == tf.estimator.ModeKeys.TRAIN, use_batchnorm, data_format, name)\n",
    "    # This acts as a no-op if the logits are already in fp32 (provided logits are\n",
    "    # not a SparseTensor). If dtype is is low precision, logits must be cast to\n",
    "    # fp32 for numerical stability.\n",
    "    logits = tf.cast(outputs, tf.float32)\n",
    "\n",
    "    predictions = {\n",
    "      'classes': tf.argmax(logits, axis=1),\n",
    "      'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # Return the predictions and the specification for serving a SavedModel\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions,\n",
    "            export_outputs={\n",
    "                'predict': tf.estimator.export.PredictOutput(predictions)\n",
    "            })\n",
    "\n",
    "    # Calculate loss, which includes softmax cross entropy and L2 regularization.\n",
    "    cross_entropy = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels)\n",
    "\n",
    "    # Create a tensor named cross_entropy for logging purposes.\n",
    "    tf.identity(cross_entropy, name='cross_entropy')\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "\n",
    "    # Add weight decay to the loss.\n",
    "    l2_loss = weight_decay * tf.add_n(\n",
    "      # loss is computed using fp32 for numerical stability.\n",
    "      [tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()])\n",
    "    tf.summary.scalar('l2_loss', l2_loss)\n",
    "    loss = cross_entropy + l2_loss\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "        learning_rate = learning_rate_fn(global_step)\n",
    "\n",
    "        # Create a tensor named learning_rate for logging purposes\n",
    "        tf.identity(learning_rate, name='learning_rate')\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "        optimizer = tf.train.MomentumOptimizer(\n",
    "            learning_rate=learning_rate,\n",
    "            momentum=momentum\n",
    "        )\n",
    "\n",
    "        if loss_scale != 1:\n",
    "            # When computing fp16 gradients, often intermediate tensor values are\n",
    "            # so small, they underflow to 0. To avoid this, we multiply the loss by\n",
    "            # loss_scale to make these tensor values loss_scale times bigger.\n",
    "            scaled_grad_vars = optimizer.compute_gradients(loss * loss_scale)\n",
    "\n",
    "            # Once the gradient computation is complete we can scale the gradients\n",
    "            # back to the correct scale before passing them to the optimizer.\n",
    "            unscaled_grad_vars = [(grad / loss_scale, var)\n",
    "                                for grad, var in scaled_grad_vars]\n",
    "            minimize_op = optimizer.apply_gradients(unscaled_grad_vars, global_step)\n",
    "        else:\n",
    "            minimize_op = optimizer.minimize(loss, global_step)\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        train_op = tf.group(minimize_op, update_ops)\n",
    "    else:\n",
    "        train_op = None\n",
    "\n",
    "    if not tf.contrib.distribute.has_distribution_strategy():\n",
    "        accuracy = tf.metrics.accuracy(labels, predictions['classes'])\n",
    "    else:\n",
    "        # Metrics are currently not compatible with distribution strategies during\n",
    "        # training. This does not affect the overall performance of the model.\n",
    "        accuracy = (tf.no_op(), tf.constant(0))\n",
    "\n",
    "    metrics = {'accuracy': accuracy}\n",
    "\n",
    "    # Create a tensor named train_accuracy for logging purposes\n",
    "    tf.identity(accuracy[1], name='train_accuracy')\n",
    "    tf.summary.scalar('train_accuracy', accuracy[1])\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fn = lambda features, labels, mode :run_model_fn(features, labels, mode, \n",
    "                                                          resnet_rnn, \"resnet_rnn\", True, \n",
    "                                                          False, \"channels_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'tmp/test_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n",
      "  allow_growth: true\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8a9643d438>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# hardware configuration\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "run_config = tf.estimator.RunConfig(session_config=config)\n",
    "\n",
    "# Definition of Estimators\n",
    "classifier = tf.estimator.Estimator(model_fn=model_fn, \n",
    "                                    model_dir=\"tmp/test_model\", \n",
    "                                    config=run_config,\n",
    "                                    params=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(input_fn, max_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with RNN\n",
    "\n",
    "### Exploding/Vanishing Gradients\n",
    "\n",
    "For $$h_t = \\tanh(W_{hh}h_{t-1} + W_{xh}x_t)$$ and $$y_t = W_{hy}h_t$$.\n",
    "\n",
    "Using the Jacobians:\n",
    "\n",
    "$$J_{L}(h_{t-1}) = J_L(h_t) J_{H_t}(h_{t-1})$$\n",
    "\n",
    "$$J_{h_t}(h_{t-1}) = \\frac{1}{\\cosh^2 \\hat{h}}W_{hh}\n",
    "\n",
    "Since $\\frac{1}{\\cosh^2 \\hat{h}} \\leq 1$, $W_{hh}$ can be arbitrarily large/small.  \n",
    "\n",
    "Since $W_{hh}$ is multiplied in the gradient derivation step, gradient is a power of $W_hh$.\n",
    "\n",
    "If $\\Lambda^{\\max}_{W_hh} > 1$, gradient will grow exponentially (exploding).\n",
    "\n",
    "If $\\Lambda^{\\max}_{W_hh} < 1$, gradient will shrink exponentially (vanishing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting Topics\n",
    "1. DRAW: A Recurrent Neural Network For Image Generation, Gregor et al.\n",
    "2. Multiple Object Recognition with Visual Attention, Ba et al. \n",
    "3. Recurrent Network of Attention\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
