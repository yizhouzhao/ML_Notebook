{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Model of Visual Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Premise\n",
    "\n",
    "Instead of using a convolutional layer to process the image, recurrent model of attention utilizes sequential processing of selected regions using RNN.  Combining with reinforcement learning techniques, RVA allows the computational complexity to be independent of image size.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glimpse Sensor/Glimpse Network\n",
    "\n",
    "#### Input: \n",
    "- glimpse coordinate $l_{t-1}$ \n",
    "- image $x_t$\n",
    "\n",
    "#### Output\n",
    "- glimpse vector with high resolution at l and prograssively lower\n",
    "- lower dimensionality than $x^t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$g_t = f_g(x_t, l_{t-1}; \\theta_g)$$\n",
    "\n",
    "$$\\theta_g = \\{\\theta_g^0, \\theta_g^1, \\theta_g^2\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![glimpse network](glimpse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Internal State\n",
    "\n",
    "Internal state behaves like a recurrent neural network which takes in previous state $h_{t-1}$ and outputs $h_t$ with the following calculation:\n",
    "\n",
    "$$h_t = f_h(h_{t-1}, g_t; \\theta_h)$$\n",
    "\n",
    "![internal state](rec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "\n",
    "There are two types of actions to be performed:\n",
    "1. How to deploy sensor via sensor control $l_t$\n",
    "2. $\\alpha_t$ which affects the environment\n",
    "\n",
    "We will use a location network $f_l(h_t; \\theta_l)$, which arameterize a distribution at time $t$.  We will sample $l_t$ from this distribution to input into next time step.\n",
    "\n",
    "The environmnet action $\\alpha_t$ is drawn from a distriution conditioned on $f_{\\alpha}(h_t; \\theta_{\\alpha})$\n",
    "\n",
    "![output](output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "\n",
    "Given an action $\\alpha_t$, we will receive a new image $x_{t+1}$ and reward $r_{t+1}$.  Definition of rewards is based on the problem premise.  For example, for image recongition, we will define $r_T = 1$ if the object is correctly identified after $T$ steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at Partial Obsrevable Markov Decision Process in Reinforcement Learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
