{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Premise\n",
    "\n",
    "Given the following loss term:\n",
    "\n",
    "$$L(f(x, W), y)$$\n",
    "\n",
    "We want to compute the gradient with chain rule:\n",
    "\n",
    "$$\\frac{dL}{dW} = \\frac{dL}{df}\\frac{df}{dW} \\ \\ \\text{ or } \\ \\ \\nabla_W L = \\frac{dL}{df}\\nabla_W{f}$$\n",
    "\n",
    "if *W* is a vector of parameters.\n",
    "\n",
    "**So... what's the problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all: **Computing gradient is very expensive in a neural network**.\n",
    "\n",
    "Let $W$ be the input:\n",
    "\n",
    "$$W \\rightarrow A \\rightarrow B \\rightarrow C \\rightarrow \\cdots K \\rightarrow L$$\n",
    "\n",
    "In this case, loss function with respect to $W$ is\n",
    "\n",
    "$$L(W) = L(K \\cdots C (B(A(W))))$$\n",
    "\n",
    "Per Chain Rule, Jacobian with respect to $L$ is\n",
    "\n",
    "$$J_{L}(W) = J_{L}(K)J_{C}(B)J_{B}(A)J_{A}(W)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, **Backpropagation** is to evaluate the jacobian product fright output of neural network to towards its input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we use Backpropogation?\n",
    "\n",
    "#### Efficiency\n",
    "![](backprop1.png)\n",
    "\n",
    "Note every multiplication will result in a row vector.\n",
    "![](backprop2.png)\n",
    "\n",
    "![](backprop3.png)\n",
    "\n",
    "![](backprop4.png)\n",
    "\n",
    "So, the computational cost of multplying a row vector with matrix is $\\mathcal{O}(n^2)$ while cost of multiplying two matrices is $\\mathcal{O}(n^3)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Subexpressions\n",
    "\n",
    "![](backprop5.png)\n",
    "\n",
    "Note values such as $J_C(B)$ is also required in order to perform gradient update.  Computing it before $J_B(A)$ and $J_A(W)$ and save the value will reduce both memory and computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Backpropogation Recipies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss\n",
    "\n",
    "Cross Entropy Loss:\n",
    "\n",
    "$$L = -\\log s_y \\text{ for } y \\in \\{1, \\cdots, n\\} $$  \n",
    "\n",
    "Then\n",
    "\n",
    "$$J_{L}(s)_i = \\nabla_s L_i^T =\\begin{cases}\n",
    "    -\\frac{1}{s_i},& \\text{for } i = y\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases} $$\n",
    "\n",
    "$$J_{L}(s)_i = \\nabla_s L_i^T = -\\frac{y_i}{s_i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass SVM Loss\n",
    "\n",
    "Multiclass SVM Loss:\n",
    "\n",
    "$$L = \\sum_{i=1}^n \\max(0, 1-s_y + s_i)$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$J_{L}(s)_i = \\nabla_s L_i^T =\\begin{cases}\n",
    "    1 & \\text{if } s_y - s_i < 1 \\text{ and } i \\neq y\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Logistic Function\n",
    "\n",
    "For logistic function (softmax), \n",
    "\n",
    "$$y_i = \\frac{\\exp s_i}{\\sum_{j=1}^n \\exp s_j} = \\frac{f}{g}$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\\frac{\\partial y_i}{\\partial s_j} = y_i\\delta_{ij} - y_iy_j$$\n",
    "\n",
    "$$\\delta_{ij} = \\begin{cases}\n",
    "    1 & \\text{if } i = j\\\\\n",
    "    0,              & \\text{otherwise}\n",
    "\\end{cases}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Multiply (FC) Layer\n",
    "\n",
    "For FC Layer, the output is compute as follows:\n",
    "\n",
    "$$y_i = \\sum_{j=1}^n W_{ij}s_j$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\\frac{\\partial y_i}{\\partial s_j} = W_{ij}$$\n",
    "\n",
    "$$J_L(s) = J_L(y)W$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial W_{ik}} = J_L(y)^Ts^T$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
