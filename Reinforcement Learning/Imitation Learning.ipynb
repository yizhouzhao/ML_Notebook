{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imitation Learning**:  \n",
    "\n",
    "Given a dataset of $o_t, a_t$, imitation learning send it to supervised learning algorithm  and output $\\pi_{a_t | o_t}$\n",
    "\n",
    "![](imit_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, this approach should not work.  Why?  \n",
    "\n",
    "Since we are matching one input $(o_t, u_t)$ with one policy $\\pi_{\\theta}(u_t | o_t)$, errors may aggregate if the next state deviates from the learning trajectory.\n",
    "\n",
    "![](traj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it works?  (Perhaps because there are more data?)\n",
    "\n",
    "![](nvidia-net.png)\n",
    "\n",
    "Note the three cameras aggregate the dataset because you can adjust the action value corresponding to each camera.  (i.e. Perhaps add a left turn bias on left camera)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Premise\n",
    "\n",
    "Given the following trajectories,\n",
    "\n",
    "![](traj2.png)\n",
    "\n",
    "The problem in the previous setting is  training on policy data $p_{\\pi}$ rather than real data while $p_{data} = p_{\\pi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAggar: Dataset Ａｇgregation\n",
    "\n",
    "The goal for **DAggar** is to collect training data from $p_{\\pi_{\\theta}}(o_t)$ instead of $p_{data}(o_t)$\n",
    "\n",
    "1. train $\\pi_{\\theta}(u_t | o_t)$ from human data $\\mathcal{D} = \\{o_1, u_1, \\cdots, o_N, u_N\\}$\n",
    "2. run $\\pi_{\\theta}(u_t | o_t)$ to get the dataset $\\mathcal{D} = \\{o_1, \\cdots, o_M\\}$\n",
    "3. Ask human to label $\\mathcal{D}_{\\pi}$ with actions $u_t$\n",
    "4. Aggregate: $\\mathcal{D} \\leftarrow \\mathcal{D} \\cup \\mathcal{D}_{\\pi}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Adaption\n",
    "\n",
    "- We build a network that accounts for target domain being different from simulations.\n",
    "- In order to do so, we could share earlier layers, fine tunes last CNN layers, and replicates FC layers.\n",
    "\n",
    "$$\\min_{\\Theta} \\frac{1}{n_s} \\sum_{i=1}^{n_s} J(\\theta(x_i^s), y_i^s) + \\lambda \\sum_{l=l_1}^{l_2} d_k^2(\\mathcal{D}_s^l, \\mathcal{D}_t^l)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Recovery: Learning ADL tasks with an LSTM\n",
    "\n",
    "Rouhollah Rahmatizadeh, Pooya Abolghasemi, Aman Behal, Ladislau Boloni , “From  Virtual Demonstration to Real- World Manipulation Using LSTM and MDN” 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAIL: Generative Adversarial Imitation Learning\n",
    "\n",
    "**Inverse Reinforcement Learning**: estimating the user's cost function and then optimize the cost by training.  So, instead of blindly following the user, the agent are allowed more freedom in solving the ultimate control problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy regularization: define $H(\\pi) \\triangleq \\mathbb{E}_{\\pi}[- \\log \\pi(a | s)]$\n",
    "- Entropy is highest for a random policy\n",
    "- Entropy is lowest for a deterministic policy that takes a single action at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating the cost function is:\n",
    "$$\\DeclareMathOperator*{\\maxi}{maximize}$$\n",
    "$$\\maxi_{c \\in \\mathcal{C}}(\\min_{\\pi \\in H} - H(\\pi) + \\mathbb{E}_{\\pi}[c(s,a)]) - \\mathbb{E}_{\\pi_E}[c(s,a)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imitation ｌｅａｒｎing problem will be:\n",
    "\n",
    "$$\\DeclareMathOperator*{\\argmin}{argmin}$$\n",
    "$$RL(c) = \\argmin_{\\pi \\in \\Pi} - H(\\pi) + \\mathbb{E}_{\\pi}[c(s,a)]$$\n",
    "\n",
    "![](gail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behavioral Cloning\n",
    "\n",
    "Robots may learn certain ask by mimicking human actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensorimotor Learning\n",
    "\n",
    "![](sensorimotor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both perception and action are end to end network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
