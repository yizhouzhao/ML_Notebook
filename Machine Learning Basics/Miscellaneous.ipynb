{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Miscellaneous  \n",
    "\n",
    "## MSE Variance-Bias Decomposition\n",
    "\n",
    "$$MSE(\\hat{y}) = \\mathbb{E}_y[(y - \\hat{y})^2 ] = \\mathbb{E}_y[(y^2 - 2y\\hat{y} + \\hat{y}^2)] = \\mathbb{E}_y[\\hat{y}^2] - 2y\\mathbb{E}[\\hat{y}] + y^2)]$$\n",
    "\n",
    "$$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = Var[\\hat{y}] - (\\mathbb{E}_y[\\hat{y}]^2 - 2y\\mathbb{E}[\\hat{y}] + y^2) = Var[\\hat{y}] + (\\mathbb{E}[\\hat{y}] - y)^2$$\n",
    "\n",
    "$$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ = Var[\\hat{y}]  + \\text{Bias}(\\hat{y})^2$$\n",
    "\n",
    "[](./-f93b1bd8-3df9-41b5-a6ac-5ee1a0044b8c.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives with respect to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x \\in \\mathbb{R}^n$ (a column vector) and let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$.  The derivative of $f$ with respect to $x$ is the row vector:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = [\\frac{\\partial f}{\\partial x_1}, \\cdots, \\frac{\\partial f}{\\partial x_n}]$$\n",
    "\n",
    "which is the gradient of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hessian Matrix \n",
    "\n",
    "**Hessian Matrix** is the square matrix of second partial derivatives of a scalar valued function $f$.\n",
    "\n",
    "$$H(f) =   \\begin{bmatrix}\n",
    "    \\frac{\\partial^2 f}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\\n",
    "    \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2}\n",
    "  \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If the gradient is $0$, $f$ has **critical point** at $x$.  Determinant of Hessian at $x$ is **discriminant**\n",
    "    - If Determinant is 0, then x is degenerate critical point\n",
    "    - If Determinant is not 0:\n",
    "        - If Hessian is positive definite, f attains a local minimum at x\n",
    "        - If Hessian is negative definite, f attains a local maximum at x\n",
    "        - If Hessian has both positive and negative eigenvalues, x is a saddle point for f\n",
    "        - Else is inconclusive\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x \\in \\mathbb{R}^n$ (a column vector) and let $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$.  The derivative of $f$ with respect to $x$ is the $m$ by $n$ matrix.:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\begin{bmatrix}\n",
    "    \\frac{\\partial f(x)_1}{\\partial x_1} & \\cdots & \\frac{\\partial f(x)_1}{\\partial x_n} \\\\\n",
    "    \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{\\partial f(x)_m}{\\partial x_1} & \\cdots & \\frac{\\partial f(x)_m}{\\partial x_n}\n",
    "  \\end{bmatrix}$$\n",
    "\n",
    "which is the **Jacobian matrix** of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Derivative Terms\n",
    "\n",
    "$$\\frac{\\partial u^Tx}{\\partial x} = u^T$$\n",
    "\n",
    "$$\\frac{\\partial x^Tx}{\\partial x} = 2x^T$$\n",
    "\n",
    "$$\\frac{\\partial Ax}{\\partial x} = A$$\n",
    "\n",
    "$$\\frac{\\partial x^TAx}{\\partial x} = x^T(A + A^T)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LS with uncertain A matrix (NEED DOUBLE CHECK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A(\\delta)x = A_0 x + \\sum_{i=1}^P A_i x \\delta_i$\n",
    "\n",
    "Zero Mean:  $\\mathbb{E}[A(\\delta)x] = A_0 x$\n",
    "\n",
    "$$\\begin{split} \n",
    "\\mathbb{E}[x^T A\\{\\delta\\}^T A\\{\\delta\\}x] & = \\mathbb{E}[x^TA_0^TA_0x + 2 \\sum_{i=1}^P x^TA_0^TA_ix\\delta_i + \\sum_{i=1}^P\\sum_{j=1}^P x^TA_i^TA_jx \\delta_i \\delta_j] \\\\ &= x^TA_0^TA_0x + \\sum_{i=1}^P \\sigma_i ^2 x^TA_i^TA_i x\\end{split}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate LASSO\n",
    "\n",
    "$$\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}$$\n",
    "\n",
    "$$\\min_{x \\in \\mathbb{R}} f(x) = \\frac{1}{2} \\norm{ax - y}_2^2 + \\lambda \\lvert x \\rvert$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(x; \\mu, b) = \\frac{1}{2b}\\exp(-\\frac{\\lvert x - \\mu \\rvert}{b})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression\n",
    "\n",
    "grid search\n",
    "\n",
    "bootstrap method\n",
    "\n",
    "Hebb's rule\n",
    "\n",
    "Parzen Windows\n",
    "\n",
    "Centroid Method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
