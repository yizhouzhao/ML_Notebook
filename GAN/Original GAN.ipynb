{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Generative Adversial Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GAN** is composed of a generative and a discriminative network.  The purpose of Generative network is to generate data vectors indistinguishable from ground truth dataset.  Discriminative Network optimizes itself to best distinguish the generated data and real data.\n",
    "\n",
    "- GANs work best when output entropy is low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network\n",
    "\n",
    "1. Must be Differentiable\n",
    "2. REINFORCE can be used for discrete variables\n",
    "3. No invertibility\n",
    "4. Trainable for any size z\n",
    "5. x can be conditionally Gaussian\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\min_G\\max_D V(D,G) = \\mathbb{E}_{x \\sim p_{data}(x)}[\\log D(x)] + \\mathbb{E}_{z \\sim p_{z}(z)}[\\log ( 1 - D(G(x)))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Theorems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For $G$ fixed, the optimial discrminator $D$ is\n",
    "\n",
    "$$D_G^* = \\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$$\n",
    "\n",
    "2. The global minimum of the virtual training criterion $C(G)$ is achieved iff $p_g = p_{data}$.  At that point, $C(G)$ achieves the value $- \\log 4$\n",
    "\n",
    "\n",
    "### How does vector space arthmetic applied to GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Generative Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Visible Belief Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavenet\n",
    "### Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduction using Convolutional Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generative_model(inputs, data_format):\n",
    "    return inputs\n",
    "\n",
    "def discriminative_model(inputs, data_format):\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    # In this case, features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations of Generative Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Autoencoder\n",
    "\n",
    "$$\\log p(x) \\geq \\log p(x) - D_{KL}(q(z) || p(z | x)) = \\mathbb{E}_{z \\sim q} \\log p(x, z) + H(q)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disadvantages:\n",
    "1. Not asymptotoically consistent unless q is perfect\n",
    "2. Lower quality sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boltzmann Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align} p(x) &= \\frac{1}{Z}\\exp(-E(x, z)) \\\\\n",
    "       &= \\sum_{x}\\sum_{z} \\exp(-E(x, z))\\end{align}$$\n",
    "       \n",
    "- Partition function is intractable\n",
    "- Maybe estimated with Markov chain methods\n",
    "- Generating samples require Markov chains too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use SGD-like algorithm of choice (Adam) on two minibatches simultaneously:\n",
    "    - A minibatch of training examples\n",
    "    - A minibatch of generated samples\n",
    "- Optional: run k steps of one player for every step of the other player.\n",
    "\n",
    "$$J^{(D)} = - \\frac{1}{2} \\mathbb{E}_{x \\sim p_data} \\log D(x) - \\frac{1}{2}\\mathbb{E}_z \\log(1 - D(G(z)))$$\n",
    "\n",
    "$$J^{(G)} = - J^{(D)}$$\n",
    "\n",
    "- Equilibrium is a saddle point of the discriminator loss\n",
    "- Resembles Jensen-Shannon divergence\n",
    "- Generator minimizes the log-probability of the discriminator being correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two GAN games:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-saturating Game\n",
    "\n",
    "$$J^{(D)} = -\\frac{1}{2}\\mathbb{E}_{x \\sim p_{data}}[\\log D(x) - \\frac{1}{2} \\mathbb{E}_z \\log (1 - D(G(z)))]$$\n",
    "\n",
    "$$J^{(G)} = - \\frac{1}{2} \\mathbb{E}_z [\\log D(G(z))]$$\n",
    "\n",
    "- Equilibrium no longer describable with a single loss\n",
    "- Generator maximizes the log-probability of the discriminator being mistaken\n",
    "- Heuristically motivated; generated can still learn even when discriminator rejects all generator samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Game\n",
    "$$J^{(D)} = -\\frac{1}{2}\\mathbb{E}_{x \\sim p_{data}}[\\log D(x) - \\frac{1}{2} \\mathbb{E}_z \\log (1 - D(G(z)))]$$\n",
    "\n",
    "$$J^{(G)} = - \\frac{1}{2} \\mathbb{E}_z [\\exp(\\sigma^{-1}(D(G(z))))]$$\n",
    "\n",
    "- When discriminator is optimal, the generator gradient matches that of maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ＧＡＮ examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplacian Pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAPGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFOGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ｇｅnerative Model Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Given an image with multiple holes, geneartive model can reveal a face\n",
    "2. Semi-supervised Learning:  (More labels on the output given to the discriminator rather than fake/real image)\n",
    "3. Next Video Frames Prediction\n",
    "4. Unsupervised correspondence learning\n",
    "    - CycleGAN\n",
    "    - Allow to change features of an image:  day to night, horse to zebra\n",
    "    - Translation without parallel corpora?\n",
    "5. Simulate environment and training data\n",
    "6. Domain Adaption: Domain Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Games $\\supseteq$ Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nash Equilibrium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game cases\n",
    "\n",
    "1. Finite minmax\n",
    "2. Finite mixed strategy games\n",
    "3. Continuous, convex games\n",
    "4. Differential games"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
